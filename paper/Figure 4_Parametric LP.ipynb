{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 4\n",
    "This notebook contains three major parts: \n",
    "- I. initialize test instances based on instances generated from previous notebook  \n",
    "  - Note, you must run **Figure 3_step1_BaselineLP Generation** and **Figure 3_step2_(i)_Learn c part I**, before this\n",
    "- II. Experiment (lean A, b and c jointly for parametric lps using deep_inv_opt package)\n",
    "- III. Data Analysis (generating boxplot as Figure 4 in our paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (C) to Yingcong Tan, Andrew Delong, Daria Terekhov. All Rights Reserved.\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.max_open_warning'] = 0\n",
    "%matplotlib inline\n",
    "import deep_inv_opt as io\n",
    "import deep_inv_opt.plot as iop\n",
    "import random\n",
    "import time\n",
    "import scipy.optimize as opt\n",
    "from scipy.spatial import ConvexHull\n",
    "import os\n",
    "\n",
    "# Make the notebook stretch to 100% browser width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "\n",
    "as_tensor = io.as_tensor\n",
    "as_numpy = io.as_numpy\n",
    "as_str = io.as_str\n",
    "PATH = \"C:/Users/Public/Downloads/\"\n",
    "\n",
    "def seed_random(seed=0):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick check if required data have been generated\n",
    "\n",
    "#### Note that, you will need to genereate the test instances for figure3(i) since they are used in this notebook to generate PLP instances.\n",
    "Please make sure that you have done the following two steps before runing this notebook:\n",
    "- **Figure 3_step1_BaselineLP Generation.ipynb**, *completely*  \n",
    "- **Figure 3_step2_(i)_Learn c.ipynb**, *Part I*  \n",
    "The following cell will quickly check ifyou already have all the required data files from figure3(i), if no error raised, you might continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nVar_range = [2,10]\n",
    "nCons_range = [[8],[36]]\n",
    "num_lp = 50\n",
    "\n",
    "\n",
    "for i, nVar in enumerate (nVar_range):\n",
    "    for nCons in nCons_range[i]:\n",
    "        for ind_lp in range(num_lp):\n",
    "            directory = PATH + \"Deep_Inv_Opt/figure3_i/n=%d,m=%d/%svar_LP%s.txt\"%(nVar, nCons, str(nVar).zfill(2),str(ind_lp+1).zfill(2))\n",
    "            if not os.path.exists(directory):\n",
    "                raise RuntimeError(\"Test instance for Figure3_i does not exist, \\n You must run the code for generating Figure 3(i) first, that is: \\n\\t\\\"Figure 3_step1_BaselineLP Generation.ipynb\\\" complete \\n\\t\\\"Figure 3_step2_(i)_Learn c.ipynb\\\" part I, \\n because it generates the LP instances that are used in this notebookto define the PLPs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Parametric LP \n",
    "### Overview\n",
    "- PLP instance with linear function: param = w0 + w1*u\n",
    " - for each entry of c, add param(w1,w2) = w1 + w2*u\n",
    " - for each row of A, randomly select one entry to add param(w3,w4) = w3 + w4*u\n",
    " - for each entry of b, add param(w5,w6) = w5 + w6*u\n",
    "\n",
    "- Read c, A and b  from LP instances for Figure 3.(i), and then set to be $c, A, b$  \n",
    "- add parametric terms, i.e., param(w1=0,w2), param(w3=0,w4) and param(w5=0,w6) to $c, A\\;\\&\\; b$ to obtain $c_{corrupt}, A_{corrupt}, b_{corrupt}$, that is: \n",
    " - $c_{true} = c + w_1 + w_2*u$,  \n",
    " - $A_{true} = A + w_3 + w_4*u$,  \n",
    " - $b_{true} = b + w_5 + w_6*u$, \n",
    " - where $w_1 = w_3 = w_5 =0$, and $w_2, w_4\\; \\&\\; w_6 \\in U[-\\sigma, \\sigma]$  \n",
    "- find a set of feasible u value, and then computer $x_{target}$ by solving PLP($c_{true},\\; A_{true},\\; b_{true},\\; u$)\n",
    "- then add random noise ($U[-\\sigma_{corrupt}, \\sigma_{corrupt}]$) to every parameter $w_1,...,w_6$ to generate $c_{corrupt},\\; A_{corrupt},\\; b_{corrupt}$, that is, \n",
    " - $c_{corrupt} = c_{true} + w_1' + w_2' * u$,  \n",
    " - $A_{corrupt} = A_{true} + w_3' + w_4' * u$,      \n",
    " - $b_{corrupt} = b_{true} + w_5' + w_6' * u$,  \n",
    " - where, $w_1' = w_1 + noise$, ..., $w_6' = w_6 + noise$\n",
    "- Starting from $c_{corrupt},\\; A_{corrupt},\\; b_{corrupt}$, learng parameters $w_1', ..., w_6'$ using Deep_Inv_Opt package and attempt to recover the PLP so that error function is minimized (e.g., $\\min MSE(x_{predict},\\;x_{true})$ )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "- **Read_3aLP**: read data (A_ub, b_ub, c_init) from instances in figure 3(i)\n",
    "- **solve_lp**: run linprog function in the Deep_Inv_Opt package to solve a lp\n",
    "- **feasibility**: checking feasibility\n",
    "- **record_PLP**: record data of the parametric lps\n",
    " - *Problem size*, m,n = A.shape\n",
    " - *Base_LP*: c, A_ub, b_ub = LP, from 3a\n",
    " - *True Weights*: weights initialized for the true PLP, [wo, w1], where w0 =0, w1 = normal(0,sigma)\n",
    " - *w_A_coefficient*:  for w_A, also store a random coefficient (0 or 1 for 2d case) which indicates where to insert the parametric term of w0+w1*u\n",
    " - *u_data*: lowerbound (u_lb), upperbound(u_ub) and u_train (equaliy spaced between u_lb and u_ub)\n",
    " - *Target points*: x_target, given true_weights, generate corresponding true_PLP, and solve through linprog to obtain x_target.\n",
    " - *weights_corrupt*: corrupt the initial weights by adding noise $U(-\\sigma, \\sigma)$  \n",
    "- **read_PLP**: Read data from PLP file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_3aLP(filename):           \n",
    "    lines=[]    \n",
    "    with open(filename, \"rt\") as fp:\n",
    "        lines=fp.readlines()\n",
    "        for line in fp:\n",
    "            lines.append(line.rstrip('\\n')) \n",
    "    \n",
    "    m,n = np.fromstring(lines[0], dtype=int, sep=' ')\n",
    "    \n",
    "    temp = [lines[i+1: i+1+m] for i in range(len(lines)) if \"A_ub\" in lines[i]]\n",
    "    A_ub=np.zeros((m,n))\n",
    "    b_ub=np.zeros((m,1))\n",
    "    for j in range(m):\n",
    "        A_ub[j] = np.fromstring(temp[0][j], dtype=float, sep=' ')\n",
    "    \n",
    "    temp = [lines[i+1: i+1+m] for i in range(len(lines)) if \"b_ub\" in lines[i]]\n",
    "    for j in range(m):\n",
    "        b_ub[j] = np.fromstring(temp[0][j], dtype=float, sep=' ')\n",
    "        \n",
    "    temp = [lines[j+1] for j in range(len(lines)) if \"c_init\" in lines[j]]\n",
    "    c_init = np.fromstring(temp[0], dtype=float, sep=' ') \n",
    "\n",
    "    temp = [lines[j+1] for j in range(len(lines)) if \"vertex target\" in lines[j]]\n",
    "    x_target = np.fromstring(temp[0], dtype=float, sep=' ') \n",
    "\n",
    "    return (A_ub, b_ub, x_target, c_init)\n",
    "\n",
    "# given c, A and b, solve foward problem once using linprog to check the LP feasibility        \n",
    "def solve_lp(c, A,b):\n",
    "    # -1<= c <=1, domain of values for c \n",
    "    # normalize the vector so that ||c||_1 = 1\n",
    "    m, n = A.shape    \n",
    "    c_vector = io.tensor(c).view(-1,1) # for the purpose of feasibility check, no need to normalize c vector\n",
    "    A_ub = io.tensor(A)\n",
    "    b_ub = io.tensor(b).view(-1,1)\n",
    "    # solve the generated linear program min c'x st. Ax <= b, with no bounds on variables unless specified by A, b\n",
    "    sol  = io.linprog (c_vector, A_ub, b_ub, eps = 1e-7).detach().numpy()\n",
    "    if sol.size>0:\n",
    "        return True, sol\n",
    "    elif sol.size ==0:\n",
    "        return False, sol  \n",
    "    \n",
    "# given a point, and feasible region, check if the point is feasible\n",
    "def feasibility(point, A_ub, b_ub, tolerance= 6):\n",
    "    m,n = A_ub.shape\n",
    "    feasibility_ub = np.round(A_ub @ point.reshape((n,1)) - b_ub.reshape((m,1)), tolerance)\n",
    "    if (feasibility_ub <=0).all():\n",
    "        return True\n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_PLP(filename, LP, weights, w_A_coefficient, u_data, x_target, weights_corrupt ):\n",
    "    c, A_ub, b_ub = as_numpy(*LP)\n",
    "    m,n = A_ub.shape\n",
    "    w_c, w_A, w_b,  = as_numpy(*weights)\n",
    "    w_c_Corrupt, w_A_Corrupt, w_b_Corrupt = as_numpy(*weights_corrupt)\n",
    "    \n",
    "    u_lb, u_ub, u_train = u_data \n",
    "    u_range = np.array([u_lb, u_ub]).reshape((1,2))\n",
    "        \n",
    "    with open(filename, \"w\") as PLP_file:\n",
    "        np.savetxt(PLP_file, np.array(A_ub.shape).ravel(), fmt=\"%d\")\n",
    "        PLP_file.write(\"\\n\")\n",
    "        \n",
    "        PLP_file.write(\"A_ub\\n\")\n",
    "        np.savetxt(PLP_file, A_ub, fmt=\"%.6f\")\n",
    "        PLP_file.write(\"w_A\\n\")\n",
    "        np.savetxt(PLP_file, w_A.reshape((1,2)), fmt=\"%.6f\")\n",
    "        np.savetxt(PLP_file, w_A_coefficient.reshape((1, m)), fmt=\"%.6f\")\n",
    "\n",
    "        PLP_file.write(\"\\n\")        \n",
    "        PLP_file.write(\"b_ub\\n\")\n",
    "        np.savetxt(PLP_file, b_ub.reshape((1,m)), fmt=\"%.6f\")       \n",
    "        PLP_file.write(\"w_b\\n\")\n",
    "        np.savetxt(PLP_file, w_b.reshape((1,2)), fmt=\"%.6f\")\n",
    "        \n",
    "        PLP_file.write(\"\\n\")  \n",
    "        PLP_file.write(\"c_true\\n\")\n",
    "        np.savetxt(PLP_file, c.reshape((1,n)), fmt=\"%.6f\")        \n",
    "        PLP_file.write(\"w_c\\n\")\n",
    "        np.savetxt(PLP_file, w_c.reshape((1,2)), fmt=\"%.6f\")\n",
    "        \n",
    "        PLP_file.write(\"\\n\")  \n",
    "        PLP_file.write(\"u_train\\n\")\n",
    "        np.savetxt(PLP_file, u_range, fmt=\"%.6f\") \n",
    "        np.savetxt(PLP_file, u_train.reshape((1,len(u_train))), fmt=\"%.6f\") \n",
    "        \n",
    "        PLP_file.write(\"\\n\")      \n",
    "        PLP_file.write(\"x_target \\n\") \n",
    "        np.savetxt(PLP_file, np.array(x_target), fmt=\"%.6f\")\n",
    "        PLP_file.write(\"\\n\")        \n",
    "        PLP_file.write(\"w_A_Corrupt\\n\")\n",
    "        np.savetxt(PLP_file, w_A_Corrupt, fmt=\"%.6f\")\n",
    "\n",
    "        PLP_file.write(\"\\n\")        \n",
    "    \n",
    "        PLP_file.write(\"w_b_Corrupt\\n\")\n",
    "        np.savetxt(PLP_file, w_b_Corrupt, fmt=\"%.6f\")\n",
    "        \n",
    "        PLP_file.write(\"\\n\")      \n",
    "        PLP_file.write(\"w_c_Corrupt\\n\")\n",
    "        np.savetxt(PLP_file, w_c_Corrupt, fmt=\"%.6f\")\n",
    "\n",
    "# read txt file to retrive all related data for each PLP instance\n",
    "def read_PLP(filename):\n",
    "    lines=[]\n",
    "    with open(filename, \"rt\") as fp:\n",
    "        for line in fp:\n",
    "            lines.append(line.rstrip('\\n')) \n",
    "    \n",
    "    m = int(lines[0])\n",
    "    n = int(lines[1])\n",
    "    \n",
    "    print(m,n)\n",
    "    \n",
    "    temp = [lines[i+1:i+m+1] for i in range(len(lines)) if \"A_ub\" in lines[i]]\n",
    "    A_ub = []\n",
    "    for i in range(len(temp[0])):\n",
    "        te = np.fromstring(temp[0][i], dtype=float, sep=' ')\n",
    "        A_ub.append(te)\n",
    "    A_ub = np.array(A_ub)\n",
    "    \n",
    "    temp = [lines[i+1:i+3] for i in range(len(lines)) if \"w_A\" in lines[i]\n",
    "           and \"Corrupt\" not in lines[i]]\n",
    "    w_A = []\n",
    "    te = np.fromstring(temp[0][0], dtype=float, sep=' ')\n",
    "    w_A = np.array(te)\n",
    "    te = np.fromstring(temp[0][1], dtype=float, sep=' ')\n",
    "    w_A_coefficient =np.array(te).reshape((1,m))\n",
    "    \n",
    "    temp = [lines[i+1] for i in range(len(lines)) if \"b_ub\" in lines[i]]\n",
    "    te = np.fromstring(temp[0], dtype=float, sep=' ')\n",
    "    b_ub = np.array(te).reshape((m,1))\n",
    "    \n",
    "    temp = [lines[i+1] for i in range(len(lines)) if \"w_b\" in lines[i]\n",
    "           and \"Corrupt\" not in lines[i]]\n",
    "    te = np.fromstring(temp[0], dtype=float, sep=' ')\n",
    "    w_b = np.array(te)\n",
    "    \n",
    "    temp = [lines[i+1] for i in range(len(lines)) if \"c_true\" in lines[i]]\n",
    "    c_true = np.fromstring(temp[0], dtype=float, sep=' ').reshape((-1,1))\n",
    "    \n",
    "    temp = [lines[i+1] for i in range(len(lines)) if \"w_c\" in lines[i] \n",
    "            and \"Corrupt\" not in lines[i]]\n",
    "    w_c = []\n",
    "    te = np.fromstring(temp[0], dtype=float, sep=' ')\n",
    "    w_c = np.array(te)\n",
    "\n",
    "    temp = [lines[i+2] for i in range(len(lines)) if \"u_train\" in lines[i]]\n",
    "    u = []\n",
    "    for i in range(len(temp[0])):\n",
    "        te = np.fromstring(temp[0], dtype=float, sep=' ')\n",
    "    u = te.reshape((len(te),1))\n",
    "    \n",
    "    temp = [lines[i+1:i+len(u)+1] for i in range(len(lines)) if \"x_target\" in lines[i]]\n",
    "    x_target = []\n",
    "    for i in range(len(temp[0])):\n",
    "        te = np.fromstring(temp[0][i], dtype=float, sep=' ')\n",
    "        x_target.append(te)\n",
    "    x_target = np.array(x_target).reshape((len(u),n))\n",
    "    \n",
    "    \n",
    "    temp = [lines[i+1] for i in range(len(lines)) if \"w_A_Corrupt\" in lines[i]]\n",
    "    te = np.fromstring(temp[0], dtype=float, sep=' ')\n",
    "    w_A_Corrupt = np.array(te)    \n",
    "    \n",
    "    temp = [lines[i+1] for i in range(len(lines)) if \"w_b_Corrupt\" in lines[i]]\n",
    "    te = np.fromstring(temp[0], dtype=float, sep=' ')\n",
    "    w_b_Corrupt = np.array(te)\n",
    "    \n",
    "    temp = [lines[i+1] for i in range(len(lines)) if \"w_c_Corrupt\" in lines[i]]\n",
    "    te = np.fromstring(temp[0], dtype=float, sep=' ')\n",
    "    w_c_Corrupt = np.array(te)\n",
    "    \n",
    "    return (c_true, A_ub, b_ub), (w_c, w_A, w_b), (w_c_Corrupt, w_A_Corrupt, w_A_coefficient, w_b_Corrupt), x_target, u\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ParametricLP class\n",
    "\n",
    "#### See \"parametric.py\" for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ParametricLP(io.ParametricLP):\n",
    "\n",
    "    #initialize parameters\n",
    "    def __init__(self, PLP, weights, w_A_coefficient):\n",
    "        super().__init__(weights)\n",
    "        \n",
    "        c_base, A_base, b_base = PLP\n",
    "        self.c_base = io.tensor(c_base).view(-1,1)\n",
    "        self.A_base = io.tensor(A_base)\n",
    "        self.b_base = io.tensor(b_base).view(-1,1)\n",
    "        \n",
    "        self.w_A_coefficient = w_A_coefficient.ravel()\n",
    "        \n",
    "        self.weights = as_tensor(weights)\n",
    "        self.weights.requires_grad_()\n",
    "                \n",
    "    def zero_grads(self):\n",
    "        if self.weights.grad is not None:\n",
    "            self.weights.grad.detach_()\n",
    "            self.weights.grad.data.zero_()\n",
    "            \n",
    "    # unpack weights, weigths = [w_c, w_A, w_b]      \n",
    "    def unpack_weights(self, weights):\n",
    "        m, n = self.A_base.shape\n",
    "        w_c = weights[:2].view((1,2))\n",
    "        w_A = weights[2:4].view((1,2))\n",
    "        w_b = weights[4:].view((1,2))\n",
    "        return w_c, w_A, w_b\n",
    "    \n",
    "    # genearte corresponding PLP using the given u's and weights\n",
    "    def generate(self, u, weights):        \n",
    "        m,n = self.A_base.shape\n",
    "        \n",
    "        w_c, w_A, w_b = self.unpack_weights(weights)\n",
    "        \n",
    "        # add linear function to a random element of each row of A\n",
    "        A_PLP = torch.zeros((m,n),dtype=torch.double)\n",
    "        for j in range(m):\n",
    "            A_PLP[j, self.w_A_coefficient[j]] = w_A[0,0]+w_A[0,1]*u\n",
    "        A_PLP = torch.add(A_PLP, self.A_base)\n",
    "        \n",
    "        # add linear function to every element of b and c\n",
    "        b_PLP = torch.zeros((m,1),dtype=torch.double)\n",
    "        for j in range(m):\n",
    "            b_PLP[j] = w_b[0,0] + w_b[0,1]*u\n",
    "        b_PLP = torch.add (b_PLP, self.b_base)\n",
    "\n",
    "        c_PLP = torch.zeros((n,1),dtype=torch.double)\n",
    "        for i in range(n):\n",
    "            c_PLP[i] = w_c[0,0] + w_c[0,1]*u\n",
    "        c_PLP = torch.add (c_PLP, self.c_base) \n",
    "        \n",
    "        return c_PLP, A_PLP, b_PLP, None, None\n",
    "    \n",
    "    #test the PLP_true to find upperbound and lowerbound for u\n",
    "    def find_uRange(self):\n",
    "        u_range = []\n",
    "        step, u_lb, u = 0, 0, -0.05\n",
    "        print(\"\\n||| find min(u) |||\")\n",
    "        while u_lb>-1.0:\n",
    "            try:\n",
    "                c_PLP, A_PLP, b_PLP, _, _ = self.generate(u, self.weights)\n",
    "                LP_test_flag, sol = solve_lp(c_PLP, A_PLP, b_PLP)\n",
    "                if LP_test_flag and max(abs(sol))<20:\n",
    "                    if u<= u_lb: # decrease u_lb if PLP(ui) is feasible and has more than 2 active constraints\n",
    "                        u_lb = u\n",
    "                    print(\"||| step %d, GOOD PLP instance with u =%.2f\"% (step,u))\n",
    "                else:\n",
    "                    print(\"||| step %d, BAD PLP instance with u =%.2f\"% (step,u))\n",
    "                    break\n",
    "            except io.InfeasibleConstraintsError:\n",
    "                # linprog_feasible failure\n",
    "                print(\"||| step %d, PLP(u = %.2f) raise a InfeasibleConstraintsError\"% (step,u))\n",
    "                break\n",
    "            except io.UnboundedConstraintsError:\n",
    "                # linprog_feasible failure\n",
    "                print(\"||| step %d, PLP(u = %.2f) raise a UnboundedConstraintsError\"% (step,u))\n",
    "                break\n",
    "            except io.MatrixInverseError:\n",
    "                # linprog_feasible failure\n",
    "                print(\"||| step %d, PLP(u = %.2f) raise a MatrixInverseError\"% (step,u))\n",
    "                break        \n",
    "            u -=0.05\n",
    "            step+=1\n",
    "        \n",
    "        step, u_ub, u = 0, 0, 0.05\n",
    "        print(\"\\n||| find max(u) |||\")\n",
    "        while u_ub<1.0:\n",
    "            try:\n",
    "                c_PLP, A_PLP, b_PLP, _, _ = self.generate(u,self.weights)\n",
    "                LP_test_flag, sol = solve_lp(c_PLP, A_PLP, b_PLP)\n",
    "                if LP_test_flag and max(abs(sol))<20:\n",
    "                    if u>= u_ub:# increase u_ub if PLP(ui) is feasible and has more than 2 active constraints\n",
    "                        u_ub = u\n",
    "                    print(\"||| step %d, GOOD PLP instance with u =%.2f\"% (step,u))\n",
    "                else:\n",
    "                    print(\"||| step %d, BAD PLP instance with u =%.2f\"% (step,u))\n",
    "                    break\n",
    "            except io.InfeasibleConstraintsError:\n",
    "                # linprog_feasible failure\n",
    "                print(\"||| step %d, PLP(%.2f) raise a InfeasibleConstraintsError\"% (step,u))\n",
    "                break\n",
    "            except io.UnboundedConstraintsError:\n",
    "                # linprog_feasible failure\n",
    "                print(\"||| step %d, PLP(%.2f) raise a UnboundedConstraintsError\"% (step,u))\n",
    "                break\n",
    "            except io.MatrixInverseError:\n",
    "                # linprog_feasible failure\n",
    "                print(\"||| step %d, PLP(%.2f) raise a MatrixInverseError\"% (step,u))\n",
    "                break\n",
    "            u +=0.05\n",
    "            step+=1\n",
    "        print(\"||| bound for u = [%.2f, %.2f] |||\\n\"%(u_lb, u_ub))\n",
    "        return u_lb, u_ub\n",
    "        \n",
    "    # generate the complete PLP and record all related data using record_PLP function\n",
    "    def generate_PLP(self, num_u):\n",
    "        u_bound = self.find_uRange()\n",
    "        u_lb, u_ub = u_bound\n",
    "        m,n = self.A_base.shape            \n",
    "        print(\"\\n===== Test new True_PLP over %d u_Train=====\"%num_u)\n",
    "        # given the true PLP and u = [lb, ub] from test_uRange, generate num_u observations and their corresponding LP\n",
    "        x_target = []\n",
    "        print(\"u_range [%.2f, %.2f]\"%(u_lb, u_ub))\n",
    "        # generate equaly spaced u_train, abs(u_lb) and abs(u_ub) > 1.0, might give wired feasible region, \n",
    "        # u~[-1,1] tend to give PLP within a reasonable range with similar shape\n",
    "        u_train = np.round(np.linspace( max(-1,u_lb), min(1,u_ub), num_u),3) \n",
    "        print(\"u_train\", u_train)\n",
    "        u_data = (u_lb, u_ub, u_train)\n",
    "        \n",
    "        # compute x_target for training\n",
    "        for ind_u, ui in enumerate (u_train):\n",
    "            temp = io.linprog(*self.generate(ui, self.weights), eps=0.0000001).detach()\n",
    "            if max(abs(temp))<20: # make sure the x_target of u_train is not located too far away from the origin\n",
    "                x_target.append(temp.numpy().ravel())            \n",
    "            else:\n",
    "                return False, None, None    \n",
    "        \n",
    "        if n<=2:\n",
    "            fig = self.plot_PLP(u_train, x_target)\n",
    "      \n",
    "            return True, np.array(x_target), u_data, fig\n",
    "        else:\n",
    "            return True, np.array(x_target), u_data\n",
    "    \n",
    "    def plot_PLP(self, u_train,x_target):\n",
    "        alpha = 0.25\n",
    "        fig = plt.figure(dpi=100, figsize=(5, 5))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.set_xlim(-5, 5)\n",
    "        ax.set_ylim(-5, 5)\n",
    "        ax.set_aspect('equal', 'box')\n",
    "        fig.tight_layout()\n",
    "        \n",
    "        for ind_u, ui in enumerate (u_train):\n",
    "            alpha_i = alpha + (1-alpha)*(ind_u/(len(u_train)-1) if len(u_train) > 1 else 1)\n",
    "            c, A_ub, b_ub, _, _ = self.generate(ui, self.weights)\n",
    "            plot_flag = iop.plot_linear_program(c,A_ub, b_ub, color='k', alpha= alpha_i,ax = ax)\n",
    "            \n",
    "        \n",
    "        iop.plot_targets(np.array(x_target), marker='o', color='k', \n",
    "                     markerfacecolor='white', markersize=10.0, \n",
    "                     markeredgewidth=1.5, alpha=0.25)\n",
    "        \n",
    "        return plt\n",
    "        \n",
    "    # given the a set of u_train's obtained above from PLP(true_weights), \n",
    "    # corrupt the true_weights so that all PLP(corrupt_weights, u_train) still corresponds to feasible LPs\n",
    "    def corrupt_PLP(self, u_train, sigma):\n",
    "        print(\"\\n\\n ====== corrupting PLP_true ======\")\n",
    "        \n",
    "        # corrupt the True Weights\n",
    "        def corrupt_weights(sigma):\n",
    "            w_c, w_A, w_b = self.unpack_weights(self.weights)\n",
    "            m,_ = w_A.shape\n",
    "            n,_ = w_c.shape  \n",
    "\n",
    "            w_A_noise = np.random.uniform(sigma*-1,sigma, (1,2))\n",
    "            w_A_corrupt = np.round( np.add(w_A.detach().numpy(), w_A_noise), 6)\n",
    "\n",
    "            w_b_noise = np.random.uniform(sigma*-1,sigma, (1,2))\n",
    "            w_b_corrupt = np.round( np.add(w_b.detach().numpy(), w_b_noise), 6)\n",
    "\n",
    "            w_c_noise = np.random.uniform(sigma*-1,sigma, (1,2))\n",
    "            w_c_corrupt = np.round( np.add(w_c.detach().numpy(), w_c_noise), 6)\n",
    "            \n",
    "            weights = np.append(np.append( w_c_corrupt.reshape(-1), \n",
    "                                          w_A_corrupt.reshape(-1)  ), \n",
    "                                w_b_corrupt.reshape(-1))\n",
    "\n",
    "            return io.tensor(weights)\n",
    "        \n",
    "        m,n = self.A_base.shape\n",
    "        flag = False\n",
    "\n",
    "        while flag == False: # regenerate new corrupted weights if any of the corrupt weights gives infeasible LPs \n",
    "            print(\"\\n===== generate new corrupted weights =====\")\n",
    "            weights_corrupt = corrupt_weights(sigma)\n",
    "\n",
    "            x_init = []\n",
    "            i=0\n",
    "            feasibleLP = True\n",
    "            while i <len(u_train): #check all ui in u_train\n",
    "                print(\"***** Observation %d, ui = %.3f\"%(i, u_train[i]))\n",
    "                c_Corrupt, A_Corrupt, b_Corrupt, _, _ = self.generate(u_train[i], weights_corrupt)\n",
    "                try:\n",
    "                    # solve the forward problem with PLP_corrupt(corrupt_weights, u_train[i])\n",
    "                    x_init.append( io.linprog(io.tensor(c_Corrupt).view(-1,1),\n",
    "                                            io.tensor(A_Corrupt),\n",
    "                                            io.tensor(b_Corrupt).view(-1,1),\n",
    "                                            eps=0.0000001).detach().numpy().ravel())\n",
    "\n",
    "                    if max(abs(x_init[i]))>20:\n",
    "                        print(\"||| PLP with corrupt weights has large x_init\")\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"++++++++++ find a feasible x_init: \", x_init[i])\n",
    "                        if i == len(u_train)-1: # leave the while loop, if all ui in u_train are checked\n",
    "                            flag = True\n",
    "                        i+=1\n",
    "                except io.InfeasibleConstraintsError:\n",
    "                    print(\"||| PLP with corrupt weights raise a InfeasibleConstraintsError\"% u_train[i])\n",
    "                    break\n",
    "                except io.UnboundedConstraintsError:\n",
    "                    print(\"||| PLP with corrupt weights raise a UnboundedConstraintsError\"% u_train[i])\n",
    "                    break\n",
    "                except io.MatrixInverseError:\n",
    "                    print(\"||| PLP with corrupt weights raise a MatrixInverseError\"% u_train[i])\n",
    "                    break\n",
    "        print(\"corrupted weights = \\n\", (*weights_corrupt.numpy()))\n",
    "\n",
    "        return weights_corrupt\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Generate PLP instances\n",
    "\n",
    "run the following cell to generate complete test instances for Parametric LP case:\n",
    "#### Note, for PLP, we only generate instances with two different sizes: 2D with 8 constraints and 10D with 36 constraints, 50 instances each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed_random(0)\n",
    "sigma = 0.2 # for generating the true weights\n",
    "corrupt_sigma = 0.2 # for corrupting the weights\n",
    "num_u = 20\n",
    "num_lp = 50\n",
    "\n",
    "nVar_range = [2,10]\n",
    "nCons_range = [[8],\n",
    "               [36]]\n",
    "\n",
    "for i, nVar  in enumerate (nVar_range):\n",
    "    for nCons in nCons_range[i]:\n",
    "        print(nVar, nCons)\n",
    "        for ind_lp in range(num_lp):\n",
    "            LP_filename = PATH + \"Deep_Inv_Opt/figure3_i/n=%d,m=%d/%svar_LP%s.txt\"%(nVar, nCons, str(nVar).zfill(2), str(ind_lp+1).zfill(2)) \n",
    "            # reading from 3a instances\n",
    "            A_ub, b_ub, _, c = Read_3aLP(LP_filename)\n",
    "            print(\"c\\n\",c)\n",
    "            print(\"A\\n\",A_ub)  \n",
    "            print(\"b\\n\",b_ub.ravel())\n",
    "        \n",
    "            flag = False\n",
    "            while flag == False:\n",
    "                #Generate true_PLP, w0 = 0, and w1 = normal(0, sigma)\n",
    "                w_c = np.round( np.column_stack( (np.zeros((1,1)), np.random.normal(0, sigma, 1)) ), 6)\n",
    "                w_A = np.round( np.column_stack( (np.zeros((1,1)), np.random.normal(0, sigma, 1)) ), 6)\n",
    "                w_b = np.round( np.column_stack( (np.zeros((1,1)), np.random.normal(0, sigma, 1)) ), 6)\n",
    "                w_A_coefficient = np.random.randint(0, nVar, nCons)\n",
    "\n",
    "                print(\"w_c\\n\",w_c)\n",
    "                print(\"w_A_coefficient\\n\", w_A_coefficient.ravel())\n",
    "                print(\"w_A\\n\",w_A)  \n",
    "                print(\"w_b\\n\",w_b)\n",
    "\n",
    "                print()\n",
    "                true_weights = np.append(np.append( w_c.reshape(-1), w_A.reshape(-1)  ), w_b.reshape(-1))\n",
    "                base_LP = (c, A_ub, b_ub)\n",
    "\n",
    "                PLP_true = ParametricLP(base_LP, true_weights, w_A_coefficient)\n",
    "                print()\n",
    "\n",
    "                print(\"==== Generate a set of observations (u's) ====\")\n",
    "                if nVar <=2:\n",
    "                    flag, x_target, u_data, plt = PLP_true.generate_PLP(num_u = num_u)\n",
    "                else: \n",
    "                    flag, x_target, u_data = PLP_true.generate_PLP(num_u = num_u)\n",
    "                if flag == True:\n",
    "                    print(\"generate a good PLP_true with 20 u's\")\n",
    "                    print(\"x_target of PLP_true\", x_target)\n",
    "                    \n",
    "                    directory = PATH + \"Deep_Inv_Opt/figure4_plp/n=%d,m=%d/\"%(nVar, nCons)\n",
    "                    if not os.path.exists(directory):\n",
    "                        os.makedirs(directory)\n",
    "                    \n",
    "                    if nVar <= 2:\n",
    "                        figname = directory + \"%svar_LP%s_True\"%(str(nVar).zfill(2), str(ind_lp+1).zfill(2)) \n",
    "                        plt.savefig(figname, bbox_inches='tight', dpi=100)\n",
    "                    \n",
    "            u_lb, u_ub, u_train = u_data\n",
    "\n",
    "            weights_corrupt = PLP_true.corrupt_PLP (u_train, sigma = corrupt_sigma)\n",
    "            \n",
    "            PLP_filename = directory + \"%svar_LP%s.txt\"%(str(nVar).zfill(2), str(ind_lp+1).zfill(2)) \n",
    "            # record all relavent data for the PLP\n",
    "            record_PLP(PLP_filename, (PLP_true.c_base, PLP_true.A_base, PLP_true.b_base), \n",
    "                       (PLP_true.unpack_weights(PLP_true.weights)), \n",
    "                       PLP_true.w_A_coefficient, \n",
    "                       u_data, x_target,\n",
    "                       (PLP_true.unpack_weights(weights_corrupt)) ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Experiment - Learn A, b and c jointly for Parametric LP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_RANGE_CONS = [[4,  8,  16],\n",
    "               [20, 36, 80]]\n",
    "_RANGE_VAR = [2, 10]\n",
    "\n",
    "_RANGE_MU= [1.5, 2, 5, 10, 20]\n",
    "_RANGE_T0 = [0.5, 1, 5, 10]\n",
    "_RANGE_LR_Ab = [1, 10]\n",
    "_LR_C_AB_RATIO = [100, 1, 0.01]\n",
    "_RANGE_EPS = [1e-5]\n",
    "_LOSS_FUNCTION = [\"MSE\"]\n",
    "_HYPERPARAM = (_RANGE_MU, _RANGE_T0, _RANGE_EPS, _RANGE_LR_Ab,_LR_C_AB_RATIO)\n",
    "\n",
    "\n",
    "def sample_hyperparam(filename, num_set = 20, print_hyperparam = True):\n",
    "    hyperParam = [[]]*num_set\n",
    "    for i in range(num_set):\n",
    "        mu = _RANGE_MU[np.random.randint(0, len(_RANGE_MU))]\n",
    "        t0 = _RANGE_T0[np.random.randint(0, len(_RANGE_T0))]\n",
    "        eps = _RANGE_EPS[np.random.randint(0, len(_RANGE_EPS))]\n",
    "        \n",
    "        lr_Ab = _RANGE_LR_Ab[np.random.randint(0, len(_RANGE_LR_Ab))]\n",
    "        lr_c = lr_Ab*_LR_C_AB_RATIO[np.random.randint(0, len(_LR_C_AB_RATIO))]\n",
    "        hyperParam[i] = (mu, t0, eps, lr_c, lr_Ab)\n",
    "        if print_hyperparam:\n",
    "            print(\"hyperparam set %d\"%(i))\n",
    "            print(hyperParam[i])\n",
    "        \n",
    "    with open(filename, 'w') as file:\n",
    "        for j in range(len(hyperParam)):\n",
    "            file.write(\"hyperParam%s: \"%(str(j+1).zfill(2)))\n",
    "            np.savetxt(file, np.mat(hyperParam[j]).ravel(), fmt='%.6f')\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "def read_hyperParam(filename, num_hyper = 20):\n",
    "    with open(filename, 'rt') as file:\n",
    "        lines=file.readlines()\n",
    "        temp = [lines[i] for i in range(len(lines)) if \"hyperParam\" in lines[i]]\n",
    "        hyperParam = [[]]*num_hyper\n",
    "        for i in range(len(temp)):\n",
    "            te = temp[i]\n",
    "            te = te[14:]\n",
    "            te = te[:-2]\n",
    "            te = te.split(' ')\n",
    "            hyperParam[i] = [float(j) for j in te]\n",
    "    return hyperParam\n",
    "\n",
    "print(\"==== Sample Hyperparameters values for Random Hyperparameter Search====\")\n",
    "seed_random()\n",
    "filename = PATH + \"Deep_Inv_Opt/Figure4_plp/Hyper_Param.txt\"\n",
    "sample_hyperparam(filename)\n",
    "read_hyperParam(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run the following cell for experiment of learning Parametric LPs using Deep_Inv_Opt package\n",
    "\n",
    "#### Please Note, finishing the entire experiments (i.e., 2 * 50 instances for Training with MSE) might take 1-2 days. You can see our paper for the experiment results directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def exp_PLP(nVar, nCons, ind_lp, hyperParam,num_u):\n",
    "    PLP_filename = PATH + \"Deep_Inv_Opt/figure4_plp/n=%d,m=%d/%svar_LP%s.txt\"%(nVar, nCons, str(nVar).zfill(2), str(ind_lp+1).zfill(2)) \n",
    "    print(filename)\n",
    "    base_PLP, weights_true, weights_corrupt, x_target, u = read_PLP(PLP_filename)\n",
    "    c, A_ub, b_ub = base_PLP\n",
    "    w_c, w_A, w_b = weights_true\n",
    "    w_c_corrupt, w_A_corrupt, w_A_coefficient, w_b_corrupt = weights_corrupt\n",
    "\n",
    "    w_A_coefficient = w_A_coefficient.astype(int)\n",
    "\n",
    "    print(\"w_c\\n\",w_c)\n",
    "    print(\"w_A_coefficient\\n\", w_A_coefficient.ravel())\n",
    "    print(\"w_A\\n\",w_A)  \n",
    "    print(\"w_b\\n\",w_b)\n",
    "\n",
    "    print(\"w_c_corrupt\\n\",w_c_corrupt)\n",
    "    print(\"w_A_corrupt\\n\",w_A_corrupt)  \n",
    "    print(\"w_b_corrupt\\n\",w_b_corrupt)\n",
    "\n",
    "    print()\n",
    "\n",
    "    #initialize all data matrix\n",
    "    record_loss=9999\n",
    "    \n",
    "    corrupt_weights = np.append(np.append( w_c_corrupt.reshape(-1), w_A_corrupt.reshape(-1) ), w_b_corrupt.reshape(-1))\n",
    "\n",
    "    result_file = PATH+\"Deep_Inv_Opt/figure4_plp/FinalExp _%svar_%scon_%s.txt\"%(str(nVar).zfill(2),str(nCons).zfill(2), \"MSE\")\n",
    "    with open(result_file, 'a') as file:\n",
    "        # print LP size, m - # constrains, n - # # var\n",
    "        file.write( \"LP%s, %svar %scon:\\n\"\n",
    "                   %(str(ind_lp+1).zfill(2),str(nVar).zfill(2),str(nCons).zfill(2)))\n",
    "        file.write( \"x_target \\n\" ) \n",
    "        np.savetxt(file, x_target.reshape(num_u,nVar), fmt='%.6f')\n",
    "        file.write(\"weights_init: \")\n",
    "        np.savetxt(file, corrupt_weights.reshape(1,len(corrupt_weights)), fmt='%.6f')\n",
    "    \n",
    "    hyperparam_flag=False\n",
    "    for ind_search in range(len(hyperParam)):\n",
    "        print(\"---- EXP %d / %d: \"%(ind_search+1, len(hyperParam)))\n",
    "\n",
    "        if hyperparam_flag == False:\n",
    "            print(\"Have not found a hyperparam set give err < 1e-4\")\n",
    "            # initialize hyper parameters\n",
    "\n",
    "            # assign different learning rate\n",
    "            # to use this, one must re-write the sample_hyperparam() function in the previous cell\n",
    "            mu, t0, eps, lr_c, lr_Ab = hyperParam[ind_search]\n",
    "            Globallr = [1 for i in range(len(corrupt_weights))]\n",
    "            for ind_lr in range(len(Globallr)):\n",
    "                if ind_lr<2:\n",
    "                    Globallr[ind_lr] = lr_c\n",
    "                else:\n",
    "                    Globallr[ind_lr] = lr_Ab\n",
    "            #initialize the corrupt_weights\n",
    "            corrupt_weights = np.append(np.append( w_c_corrupt.reshape(-1), w_A_corrupt.reshape(-1) ), w_b_corrupt.reshape(-1))\n",
    "\n",
    "            # comment the following lines to disable some printing along problem solving\n",
    "            print(\"---- corrupt_weights\",corrupt_weights)\n",
    "            print(\"---- x_target\", x_target)\n",
    "            print(\"---- learning rate\",Globallr)\n",
    "            print(\"loss func MSE, HyperParam %d = (mu:%.1f, t0:%.1f, eps:%.5f)\"\n",
    "                  %(ind_search+1, mu, t0, eps))\n",
    "            \n",
    "            base_LP = (c, A_ub, b_ub)\n",
    "            PLP = ParametricLP (base_LP, corrupt_weights, w_A_coefficient)\n",
    "            u_train = io.tensor(u)\n",
    "            x_target = io.tensor(x_target)\n",
    "            start_time = time.time()    \n",
    "            max_steps = 200\n",
    "            f_learned, err_final, x_final = io.inverse_parametric_linprog(u_train, x_target, PLP, lr=Globallr, \n",
    "                                                                          max_steps=max_steps,\n",
    "                                                                          eps = eps,\n",
    "                                                                          eps_decay=False, # change eps_callback to using eps_decay\n",
    "                                                                          callback=io.inverse_parametric_linprog_step_printer(),\n",
    "                                                                          solver=io.custom_linprog(t0 = t0, mu=mu))\n",
    "            \n",
    "            runtime = time.time() - start_time # compute runtime\n",
    "\n",
    "            w = f_learned.weights.data.detach().numpy()\n",
    "            x = x_final.detach().numpy().ravel()\n",
    "            l = err_final.detach().numpy().ravel()\n",
    "            t = runtime\n",
    "\n",
    "            # for each hyperparam search, record the experimen data:\n",
    "            with open(result_file, 'a') as file:\n",
    "                    # values of hyperparameters, final error, and runtime\n",
    "                    if len(hyperParam[0]) == 5:\n",
    "                        temp = (mu, t0, eps, lr_c, lr_Ab, l[0], t)\n",
    "                    string = (\"hyperparam\"+str(ind_search+1).zfill(2)+\" \"+ str(temp) )\n",
    "                    file.write(str(string))\n",
    "                    file.write(\"\\n\")\n",
    "                    # record final_weights\n",
    "                    file.write(\"weights_final \")\n",
    "                    np.savetxt(file, w.reshape((1, len(w))), fmt='%.6f') \n",
    "                    # record x_final\n",
    "                    file.write(\"x_final \")\n",
    "                    np.savetxt(file, x.reshape(num_u,nVar), fmt='%.6f') \n",
    "\n",
    "            print(\"final_err\", err_final.detach().numpy().ravel())\n",
    "\n",
    "            #tracking the best-found error from hyperparam search, this is not recorded in data file\n",
    "            if err_final.detach().numpy() <= record_loss:\n",
    "                print(\"--- :) find better loss \\n\")\n",
    "                record_loss = err_final.detach().numpy().ravel()\n",
    "                best_hyperparam = ind_search\n",
    "            else:\n",
    "                print(\"--- :( no better loss found\\n\")\n",
    "            if record_loss<1e-4:\n",
    "                hyperparam_flag =True\n",
    "        elif hyperparam_flag == True:\n",
    "            print(\"Found a hyperparam set give err < 1e-4\")\n",
    "            print(\"skip the rest of hyperparam search, and record dummy experiment data\")\n",
    "            \n",
    "             # for each hyperparam search, record the experimen data:              \n",
    "            with open(result_file, 'a') as file:\n",
    "                    # values of hyperparameters, final error, and runtime\n",
    "                    mu, t0, eps, lr_c, lr_Ab = hyperParam[ind_search]\n",
    "                    temp = (mu, t0, eps, lr_c, lr_Ab, 9999, 9999)\n",
    "                    string = (\"hyperparam\"+str(ind_search+1).zfill(2)+\" \"+ str(temp) )\n",
    "                    file.write(str(string))\n",
    "                    file.write(\"\\n\")\n",
    "                    # record final_weights\n",
    "                    file.write(\"weights_final \")\n",
    "                    np.savetxt(file, np.zeros((1, len(w))), fmt='%.2f') \n",
    "                    # record x_final\n",
    "                    file.write(\"x_final\\n\")\n",
    "                    np.savetxt(file, np.zeros((num_u,nVar)), fmt='%.2f') \n",
    "\n",
    "\n",
    "filename = PATH + \"Deep_Inv_Opt/Figure4_plp/Hyper_Param.txt\"\n",
    "hyperParam = read_hyperParam(filename)\n",
    "num_u = 20  # num of training set\n",
    "num_lp=1\n",
    "\n",
    "nVar_range = [2,10]\n",
    "nCons_range = [[8],[36]]\n",
    "\n",
    "for i, nVar in enumerate(nVar_range):\n",
    "    for nCons in nCons_range[i]:\n",
    "        for ind_lp in range(num_lp):    \n",
    "            exp_PLP(nVar, nCons, ind_lp, hyperParam, num_u)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Best Training Error from experiment result data (obtained in the previous cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read experiment results of hyperparameter search \n",
    "def read_finalresult(num_lp, nCons, nVar, loss=\"MSE\"):\n",
    "    # read x_target\n",
    "    x_target = []\n",
    "    result = []\n",
    "    weights_final = []\n",
    "    x_final = []\n",
    "\n",
    "    with open(PATH + \"Deep_Inv_Opt/figure4_plp/FinalExp _%svar_%scon_%s.txt\"\n",
    "              %(str(nVar).zfill(2),str(nCons).zfill(2), loss), 'rt') as file:\n",
    "        lines=file.readlines()\n",
    "    temp = [lines[i+1:i+1+5] for i in range(len(lines)) if \"x_target\" in lines[i]]\n",
    "\n",
    "    for i in range(num_lp):\n",
    "        for j in range(len(temp[i])):\n",
    "            te = temp[i][j]\n",
    "            te = te[:-2]\n",
    "            te = te.split(' ')\n",
    "            te = [float(d) for d in te]\n",
    "            x_target.append( te)\n",
    "    x_target = np.array(x_target)\n",
    "\n",
    "    temp = [lines[i] for i in range(len(lines)) if \"hyperparam\" in lines[i]]    \n",
    "    for i in range(len(temp)):\n",
    "        te = temp[i]\n",
    "        te = te[14:]\n",
    "        te = te[:-2]\n",
    "        te = te.split(',')\n",
    "        te = [float(d) for d in te]\n",
    "        result.append(te)\n",
    "    result= np.array(result)\n",
    "    # find the index of hyperparameter search with the best error value\n",
    "    best_ind = []\n",
    "    size_hyperparam = 20\n",
    "    for i in range(num_lp):\n",
    "        best_ind.append(int ( i*size_hyperparam + np.argmin(result[i*size_hyperparam:(i+1)*size_hyperparam,-2])))\n",
    "    best_ind = np.array(best_ind)\n",
    "\n",
    "    # read c_final for every trail\n",
    "    temp = [lines[i] for i in range(len(lines)) if \"weights_final\" in lines[i]]\n",
    "    for i in range(len(temp)):\n",
    "        te = temp[i]\n",
    "        te = te[14:]                \n",
    "        te = te.split(' ')\n",
    "        te = [float(d) for d in te]\n",
    "        weights_final.append(te)\n",
    "    weights_final= np.array(weights_final)\n",
    "\n",
    "\n",
    "    return result, weights_final, best_ind, x_final\n",
    "\n",
    "\n",
    "\n",
    "def TrainErr(nVar, nCons):\n",
    "    trainig_error=[]\n",
    "    result, weights_final, best_ind, x_final = read_finalresult(num_lp, nCons, nVar, loss=\"MSE\")\n",
    "    trainig_error = result[best_ind,-2]\n",
    "    Train_MSE_filename = PATH + \"Deep_Inv_Opt/figure4_plp/n=%d,m=%d/Train_MSE.txt\"%(nVar, nCons) \n",
    "    with open (Train_MSE_filename,\"a\") as file:\n",
    "        file.write(\"Best Training Error\\n\")\n",
    "        np.savetxt(file, np.array(trainig_error), fmt=\"%.3e\") \n",
    "\n",
    "        \n",
    "num_lp=50\n",
    "nVar_range = [2,10]\n",
    "nCons_range = [[8],[36]]\n",
    "\n",
    "for i, nVar in enumerate(nVar_range):\n",
    "    for nCons in nCons_range[i]:\n",
    "        TrainErr(nVar, nCons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Initial Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InitialErr(nVar, nCons, ind_lp):\n",
    "    \n",
    "    filename = PATH + \"Deep_Inv_Opt/figure4_plp/n=%d,m=%d/%svar_LP%s.txt\"%(nVar, nCons, str(nVar).zfill(2), str(ind_lp+1).zfill(2)) \n",
    "    print(filename)\n",
    "    base_PLP, weights_true, weights_corrupt, x_target, u = read_PLP(filename)\n",
    "    c, A_ub, b_ub = base_PLP\n",
    "    w_c, w_A, w_b = weights_true\n",
    "    w_c_corrupt, w_A_corrupt, w_A_coefficient, w_b_corrupt = weights_corrupt  \n",
    "    w_A_coefficient = w_A_coefficient.astype(int)\n",
    "    u_train = u\n",
    "\n",
    "    true_weights = np.append(np.append( w_c.reshape(-1), w_A.reshape(-1) ), w_b.reshape(-1))\n",
    "\n",
    "    corrupt_weights = np.append(np.append( w_c_corrupt.reshape(-1), w_A_corrupt.reshape(-1) ), w_b_corrupt.reshape(-1))\n",
    "\n",
    "    base_LP = (c, A_ub, b_ub)\n",
    "\n",
    "    # initialize the corrupt_weights\n",
    "    PLP = ParametricLP (base_LP, corrupt_weights, w_A_coefficient)\n",
    "    \n",
    "    u_train = io.tensor(u)\n",
    "    print(\"u_train\",u_train.t().numpy())\n",
    "    \n",
    "    x_target = io.tensor(x_target)\n",
    "\n",
    "    #compute x_init\n",
    "    x_init = torch.cat([io.linprog(*PLP.generate(ui, io.tensor(corrupt_weights)), eps=0.0000001).detach().t() for ui in u_train])\n",
    "    print(\"\\n======   x_init   ======\\n\",x_init.detach().numpy())\n",
    "    print(\"\\n======   x_target   ======\\n\", x_target.detach().numpy())\n",
    "    \n",
    "    x_MSE_init=[]\n",
    "    for ind_u, ui in enumerate (u_train):\n",
    "        xi_MSE_init = torch.sum(io.squared_error(None, x_target[ind_u], x_init[ind_u]), 0)\n",
    "\n",
    "        x_MSE_init.append(xi_MSE_init.detach().numpy())\n",
    "    print(\"\\n===== squared error of all u_train =====\\n\",np.array(x_MSE_init))         \n",
    "\n",
    "    \n",
    "    return np.array(x_MSE_init)\n",
    "            \n",
    "            \n",
    "alpha = 0.25\n",
    "num_lp=50\n",
    "nVar_range = [2,10]\n",
    "nCons_range = [[8],[36]]\n",
    "\n",
    "for i, nVar in enumerate(nVar_range):\n",
    "    for nCons in nCons_range[i]:\n",
    "        for ind_lp in  range(num_lp):\n",
    "\n",
    "            MSE_init = InitialErr(nVar, nCons, ind_lp)     \n",
    "            print(MSE_init)\n",
    "            init_MSE_filename = PATH + \"Deep_Inv_Opt/figure4_plp/n=%d,m=%d/Init_MSE.txt\"%(nVar, nCons) \n",
    "            with open (init_MSE_filename,\"a\") as file:\n",
    "                file.write(\"LP%s\\n\"%str(ind_lp+1).zfill(2))\n",
    "                np.savetxt(file, np.array(MSE_init).reshape((1,num_hyper)), fmt=\"%.6f\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer Test Error\n",
    "Generate a set of new u values, substitute to the trained parametric LPs, i.e., PLP($c_{train}, A_{train}, b_{train}, u_{test}$), and compute the validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_error(nVar, nCons, ind_lp, learn_weights,num_u, trainig_error):\n",
    "    filename = PATH + \"Deep_Inv_Opt/figure4_plp/n=%d,m=%d/%svar_LP%s.txt\"%(nVar, nCons, str(nVar).zfill(2), str(ind_lp+1).zfill(2)) \n",
    "    base_PLP, weights_true, weights_corrupt, _, u = read_PLP(filename)\n",
    "    c, A_ub, b_ub = base_PLP\n",
    "    print(\"base_c\",c)\n",
    "    w_c, w_A, w_b = weights_true\n",
    "    print(\"true_wc\", w_c)\n",
    "    print(\"learned_c\", learn_weights[:2])\n",
    "    _, _, w_A_coefficient, _ = weights_corrupt\n",
    "    m,n = A_ub.shape\n",
    "    w_A_coefficient = w_A_coefficient.astype(int)\n",
    "    print(filename)\n",
    "\n",
    "    print(\"u_train\",u.ravel())\n",
    "    u_lb, u_ub = u[0], u[-1]\n",
    "    u_train = u\n",
    "\n",
    "    base_LP = (c, A_ub, b_ub)\n",
    "    true_weights = io.tensor(np.append(np.append( w_c.reshape(-1), \n",
    "                                       w_A.reshape(-1) ), \n",
    "                             w_b.reshape(-1)) )\n",
    "    print(\"true_weights\", true_weights)\n",
    "\n",
    "    learn_weights = io.tensor(learn_weights.ravel())\n",
    "    print(\"learn_weights\", learn_weights)\n",
    "    \n",
    "    PLP_base = ParametricLP (base_LP, learn_weights, w_A_coefficient)\n",
    "\n",
    "    # given the true PLP and u = [lb, ub] from test_uRange, generate 4 observations and their corresponding LP\n",
    "    print(\"u_range\", u_lb, u_ub)\n",
    "    u_test = np.zeros(num_u)\n",
    "\n",
    "    x_MSE=[]\n",
    "    x_true_test=[]\n",
    "    x_learn_test=[]\n",
    "    u_test = np.random.uniform(u_lb, u_ub, num_u)\n",
    "    for i in range(len(u_test)):\n",
    "        while u_test[i] in u_train:\n",
    "            u_test[i] = np.random.uniform(max(-1,u_lb), min(1,u_ub), 1)\n",
    "    u_test.sort()\n",
    "    print(\"u_test\",u_test)\n",
    "\n",
    "    for ind_u in range(num_u):\n",
    "\n",
    "        print(\"====== new u_test\", u_test[ind_u])\n",
    "\n",
    "        xi_true_tests = io.linprog(*PLP_base.generate(u_test[ind_u], true_weights), eps=0.0000001).detach().t()\n",
    "\n",
    "        xi_learn_tests = io.linprog(*PLP_base.generate(u_test[ind_u], learn_weights), eps=0.0000001).detach().t()\n",
    "        print(\"x_learned_test\", xi_true_tests.numpy().ravel())\n",
    "        print(\"x_true_test\",xi_learn_tests.numpy().ravel())\n",
    "        \n",
    "        xi_MSE_test = torch.sum(io.squared_error(_, xi_true_tests.t(), xi_learn_tests.t()), 0)\n",
    "        print(\"SE btw learned_x, and true_x\", xi_MSE_test.numpy().ravel())\n",
    "\n",
    "        x_MSE.append(xi_MSE_test)\n",
    "        x_true_test.append(xi_true_tests.numpy().ravel())\n",
    "        x_learn_test.append(xi_learn_tests.numpy().ravel())\n",
    "\n",
    "    print(\"\\n\\n\",x_learn_test)\n",
    "    # Stack the results into matricies, for convenience\n",
    "    print(\"\\nx_MSE\\n\",torch.cat(x_MSE).numpy())\n",
    "    MSE = torch.mean(torch.cat(x_MSE))\n",
    "    u_test=np.array(u_test) \n",
    "    \n",
    "    print()\n",
    "    print(\"test_error: \", MSE.detach().numpy())     \n",
    "    print(\"best training error\",trainig_error[ind_lp])\n",
    "    print(\"====================\\n\")\n",
    "\n",
    "    return u_test, torch.cat(x_MSE).numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_hyper=20\n",
    "num_u = 20\n",
    "num_lp=50\n",
    "nVar_range = [2,10]\n",
    "nCons_range = [[8],[36]]\n",
    "\n",
    "for i, nVar in enumerate(nVar_range):\n",
    "    for nCons in nCons_range[i]:\n",
    "        result, weights_final, best_ind, x_final = read_finalresult(num_lp, nCons, nVar, loss=\"MSE\")\n",
    "        trainig_error = result[best_ind,-2]\n",
    "        learn_weights = weights_final[best_ind,:]\n",
    "        for ind_lp in  range(num_lp):\n",
    "\n",
    "            u_test, MSE_test = test_error(nVar, nCons, ind_lp, learn_weights[ind_lp], num_u, trainig_error)\n",
    "    \n",
    "            print(MSE_test)\n",
    "            test_MSE_filename = PATH+\"Deep_Inv_Opt/figure4_plp/n=%d,m=%d/Test_MSE.txt\"%(nVar, nCons) \n",
    "            with open (test_MSE_filename,\"a\") as file:\n",
    "                file.write(\"LP%s\\n\"%str(ind_lp+1).zfill(2))\n",
    "                \n",
    "                file.write(\"u_test\\n\")\n",
    "                np.savetxt(file, np.array(u_test).reshape((1,num_u)), fmt=\"%.6f\")\n",
    "                \n",
    "                file.write(\"MSE_test\\n\")\n",
    "                np.savetxt(file, np.array(MSE_test).reshape((1,num_hyper)), fmt=\"%.3e\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create boxplot as shown in Figure 4\n",
    "read all MSE data (i.e., initla, best training and test), and create boxplots\n",
    "\n",
    "**Note, the final boxplots might not look identical to what we presented in the paper due to the randomization in baseline LP generation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_MSE(nVar, nCons):\n",
    "    init_MSE_file =  PATH + \"Deep_Inv_Opt/figure4_plp/n=%d,m=%d/init_MSE.txt\"%(nVar, nCons) \n",
    "    train_MSE_file = PATH + \"Deep_Inv_Opt/figure4_plp/n=%d,m=%d/Train_MSE.txt\"%(nVar, nCons) \n",
    "    test_MSE_file =  PATH + \"Deep_Inv_Opt/figure4_plp/n=%d,m=%d/Test_MSE.txt\"%(nVar, nCons) \n",
    "    \n",
    "    init_MSE=[]\n",
    "    with open(init_MSE_file, \"rt\") as file:\n",
    "        lines=file.readlines()\n",
    "    temp = [lines[i+1] for i in range(len(lines)) if \"LP\" in lines[i]]\n",
    "    for i in range(len(temp)):\n",
    "#         te = temp[i]\n",
    "#         te = te[:-2]\n",
    "        te = np.fromstring(temp[i], dtype=float, sep=' ')\n",
    "        init_MSE.append(te)\n",
    "    init_MSE = np.array(init_MSE)\n",
    "    \n",
    "    train_MSE=[]\n",
    "    with open(train_MSE_file, \"rt\") as file:\n",
    "        lines=file.readlines()\n",
    "    temp = lines[1:]\n",
    "    for i in range(len(temp)):\n",
    "#         te = temp[i]\n",
    "#         te = te[:-2]\n",
    "        te = np.fromstring(temp[i], dtype=float, sep=' ')\n",
    "        train_MSE.append(te)\n",
    "    train_MSE = np.array(train_MSE)\n",
    "    \n",
    "    \n",
    "    test_MSE=[]\n",
    "    with open(test_MSE_file, \"rt\") as file:\n",
    "        lines=file.readlines()\n",
    "    temp = [lines[i+1] for i in range(len(lines)) if \"MSE_test\" in lines[i]]\n",
    "    for i in range(len(temp)):\n",
    "#         te = temp[i]\n",
    "#         te = te[:-2]\n",
    "        te = np.fromstring(temp[i], dtype=float, sep=' ')\n",
    "        test_MSE.append(te)\n",
    "    test_MSE = np.array(test_MSE)\n",
    "    \n",
    "    return init_MSE, train_MSE, test_MSE      \n",
    "\n",
    "\n",
    " \n",
    "    \n",
    "def box_plot(nVar, nCons):\n",
    "    init_MSE, train_MSE, test_MSE = read_MSE(nVar, nCons) \n",
    "\n",
    "    MSE_all = np.column_stack(( np.mean(init_MSE, axis = 1).reshape(num_lp,1) , \n",
    "                              np.array(train_MSE).reshape(num_lp,1), \n",
    "                              np.mean(test_MSE, axis = 1).reshape(num_lp,1) ))\n",
    "\n",
    "    for j in range(len(MSE_all[0])):\n",
    "        for i in range(len(MSE_all)):\n",
    "            if MSE_all[i,j] <1e-8:\n",
    "                MSE_all[i,j] = 1e-8\n",
    "\n",
    "    _, nbox = MSE_all.shape \n",
    "\n",
    "    y_init = MSE_all[:, 0]\n",
    "    seed_random()\n",
    "    x_init = np.random.uniform(1-.15, 1+0.15, size=len(MSE_all[:, 0]))\n",
    "    y_train = MSE_all[:, 1]\n",
    "    seed_random()\n",
    "    x_train = np.random.uniform(2-.15, 2+0.15, size=len(MSE_all[:, 1]))\n",
    "    y_test = MSE_all[:, 2]\n",
    "    seed_random()\n",
    "    x_test = np.random.uniform(3-.15, 3+0.15, size=len(MSE_all[:, 2]))\n",
    "\n",
    "    fig = plt.figure(figsize=(5,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.boxplot(MSE_all,sym='')\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.plot(x_init, y_init, 'go', mfc=\"None\", alpha=0.8) \n",
    "    ax.plot(x_train, y_train, 'bo', mfc=\"None\", alpha=0.8) \n",
    "    ax.plot(x_test, y_test, 'ro', mfc=\"None\", alpha=0.8) \n",
    "\n",
    "    ax.plot([], 'go',mfc=\"None\", alpha=0.8, label = 'Initial Err (MSE)')  \n",
    "    ax.plot([], 'bo', mfc=\"None\", alpha=0.8, label = 'Best Training Err (MSE)')\n",
    "    ax.plot([], 'ro',mfc=\"None\", alpha=0.8, label = 'Test Err(MSE)') \n",
    "    ax.set_xticks([])\n",
    "    plt.ylim(0.5e-8, 1e3)\n",
    "\n",
    "    plt.savefig(PATH+\"Deep_Inv_Opt/figure4_plp/%svar%scons_complete_MSE.pdf\"%(str(nVar).zfill(2), str(nCons).zfill(2)),\n",
    "                frameon = True, bbox_inches='tight', dpi=100)\n",
    "    plt.show()\n",
    "    \n",
    "nVar_range = [2,10]\n",
    "nCons_range = [[8],[36]]\n",
    "\n",
    "num_lp=50\n",
    "for i, nVar in enumerate(nVar_range):\n",
    "    for nCons in nCons_range[i]:\n",
    "        box_plot(nVar, nCons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
